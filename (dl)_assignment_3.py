# -*- coding: utf-8 -*-
"""(DL) Assignment 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YYfohUPvwta5Qx5oQ5YhUrtPgJDJG9LI
"""

import os
import random
import math
import time
from collections import defaultdict

import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset, Dataset
import torchvision
from torchvision import transforms
from torchvision.transforms import v2
from sklearn.model_selection import train_test_split
from torch.utils.data import Subset

from torchvision.models import resnet18, ResNet18_Weights

import pickle

CIFAR_MEAN = (0.49139968, 0.48215827, 0.44653124)
CIFAR_STD = (0.24703233, 0.24348505, 0.26158768)

# @title Transformações strong & weak

weak_transformation = v2.Compose([
    v2.RandomHorizontalFlip(),
    v2.RandomCrop(32, padding=4, padding_mode='reflect'),
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(CIFAR_MEAN, CIFAR_STD),
])

strong_transformation = v2.Compose([
    v2.RandomHorizontalFlip(),
    v2.RandomCrop(32, padding=4, padding_mode='reflect'),
    v2.RandAugment(num_ops=2, magnitude=10, interpolation=v2.InterpolationMode.BILINEAR),
    v2.RandomErasing(p=1.0, scale=(0.2, 0.4), ratio=(0.9, 1.1), value=0, inplace=True),
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(CIFAR_MEAN, CIFAR_STD),
])

test_transformation = transforms.Compose([
    v2.ToImage(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Normalize(CIFAR_MEAN, CIFAR_STD),
])

# @title Class transformations

class CIFAR10PairUnlabeled(Dataset):
    def __init__(self, base_dataset, indices, weak_transform=weak_transformation, strong_transform=strong_transformation):
        self.base = base_dataset
        self.indices = indices
        self.weak = weak_transform
        self.strong = strong_transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        image, _ = self.base[self.indices[idx]]
        w = self.weak(image)
        s = self.strong(image)
        return w, s

class CIFAR10Labeled(Dataset):
    def __init__(self, base_dataset, indices, transform):
        self.base = base_dataset
        self.indices = indices
        self.transform = transform

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        image, label = self.base[self.indices[idx]]
        return self.transform(image), label

# @title Util functions
def set_seed(seed=17):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def split_labeled_unlabeled(dataset, labels_per_class: int, seed=17):
    set_seed(seed)
    targets = np.array(dataset.targets)
    labeled_indices = []
    unlabeled_indices = []

    for c in range(10):
        class_idx = np.where(targets == c)[0]
        np.random.shuffle(class_idx)
        select = class_idx[:labels_per_class]
        rest = class_idx[labels_per_class:]
        labeled_indices.extend(select.tolist())
        unlabeled_indices.extend(rest.tolist())

    random.shuffle(labeled_indices)
    random.shuffle(unlabeled_indices)
    return labeled_indices, unlabeled_indices

# @title Import modelo ResNet 18
def get_resnet18_cifar(num_classes=10, pretrained=False, device=None):
    weights = ResNet18_Weights.DEFAULT if pretrained else None
    model = resnet18(weights=weights)

    model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    model.maxpool = torch.nn.Identity() # type: ignore
    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)
    if device:
        model.to(device)
    return model

# @title Treinametno functions
def cross_entropy_for_targets(logits, targets, reduction='mean'):
    return F.cross_entropy(logits, targets, reduction=reduction)


def train_one_epoch(model, optimizer, labeled_loader, unlabeled_loader, device, epoch,
                    lambda_u=1.0, threshold=0.95):
    model.train()
    total_loss = 0.0
    total_supervised = 0.0
    total_unsup = 0.0
    total = 0

    unlabeled_iter = iter(unlabeled_loader)

    pbar = tqdm(enumerate(labeled_loader), total=len(labeled_loader), desc=f"Epoch {epoch}")
    for _, (x_l, y_l) in pbar:
        try:
            x_uw, x_us = next(unlabeled_iter)
        except StopIteration:
            unlabeled_iter = iter(unlabeled_loader)
            x_uw, x_us = next(unlabeled_iter)

        x_l = x_l.to(device)
        y_l = y_l.to(device)
        x_uw = x_uw.to(device)
        x_us = x_us.to(device)

        batch_size = x_l.size(0)

        # labeled
        logits_x = model(x_l)
        loss_sup = F.cross_entropy(logits_x, y_l, reduction='mean')

        # unlabeled weak pseudo labels
        with torch.no_grad():
            logits_uw = model(x_uw)
            probs = torch.softmax(logits_uw, dim=1)
            max_probs, pseudo_labels = torch.max(probs, dim=1)
            mask = max_probs.ge(threshold).float()

        # unlabeled strong
        logits_us = model(x_us)

        # loss_US: CE pseudo labels and logits_us
        if mask.sum() > 0:
            loss_unsup = (F.cross_entropy(logits_us, pseudo_labels, reduction='none') * mask).mean()
        else:
            loss_unsup = torch.tensor(0.0, device=device)

        # TODO: salvar a loss_sup e loss_unsup em um arquivo
        loss = loss_sup + lambda_u * loss_unsup

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_size
        total_supervised += loss_sup.item() * batch_size
        total_unsup += loss_unsup.item() * batch_size
        total += batch_size

        pbar.set_postfix({'loss': total_loss / total, 'sup': total_supervised / total, 'unsup': total_unsup / total})

    return total_loss / total, total_supervised / total, total_unsup / total

@torch.no_grad()
def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    for x, y in loader:
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        preds = logits.argmax(dim=1)
        correct += (preds == y).sum().item()
        total += y.size(0)
    return correct / total

# @title Executar experimento auxiliar
def run_experiment(data_root, labels_per_class, save_dir, epochs=200, batch_size=64,
                   unlabeled_batch_ratio=7, lr=0.03, momentum=0.9, weight_decay=5e-4,
                   lambda_u=1.0, threshold=0.95, seed=17, device=None):

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    set_seed(seed)

    # Download CIFAR-10 train and test
    train_base = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True)
    test_set = torchvision.datasets.CIFAR10(root=data_root, train=False, transform=test_transformation, download=True)

    labeled_idx, unlabeled_idx = split_labeled_unlabeled(train_base, labels_per_class, seed=seed)

    # Split labeled dataset into train and validation
    train_labeled_idx, val_idx = train_test_split(labeled_idx, test_size=0.1, random_state=seed)

    train_labeled_ds = CIFAR10Labeled(train_base, train_labeled_idx, transform=weak_transformation)
    unlabeled_ds = CIFAR10PairUnlabeled(train_base, unlabeled_idx, weak_transform=weak_transformation, strong_transform=strong_transformation)
    val_ds = CIFAR10Labeled(train_base, val_idx, transform=test_transformation)

    train_labeled_loader = DataLoader(train_labeled_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)
    unlabeled_loader = DataLoader(unlabeled_ds, batch_size=batch_size * unlabeled_batch_ratio, shuffle=True, num_workers=2, pin_memory=False)
    val_loader = DataLoader(val_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=False)
    test_loader = DataLoader(test_set, batch_size=256, shuffle=False, num_workers=2, pin_memory=False)

    model = get_resnet18_cifar(num_classes=10, pretrained=False, device=device)
    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

    history = {'epoch': [], 'train_loss': [], 'train_sup': [], 'train_unsup': [], 'val_loss': [], 'test_acc': []}

    for epoch in range(1, epochs + 1):
        train_loss, train_sup, train_unsup = train_one_epoch(
            model, optimizer, train_labeled_loader, unlabeled_loader,
            device, epoch, lambda_u=lambda_u, threshold=threshold
        )
        scheduler.step()

        # adiciona validacao para comparação
        val_loss = 0.0
        model.eval()
        with torch.no_grad():
            for val_images, val_labels in val_loader:
                val_images, val_labels = val_images.to(device), val_labels.to(device)
                outputs = model(val_images)
                loss = torch.nn.functional.cross_entropy(outputs, val_labels)
                val_loss += loss.item()
        val_loss /= len(val_loader)

        test_acc = evaluate(model, test_loader, device)

        # === Logging ===
        history['epoch'].append(epoch)
        history['train_loss'].append(train_loss)
        history['train_sup'].append(train_sup)
        history['train_unsup'].append(train_unsup)
        history['val_loss'].append(val_loss)
        history['test_acc'].append(test_acc)

        print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, test_acc={test_acc:.4f}")

        # Quick save
        if epoch % 50 == 0:
            os.makedirs(save_dir, exist_ok=True)
            torch.save({
                'model_state': model.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'epoch': epoch
            }, os.path.join(save_dir, f'checkpoint_labels{labels_per_class}_ep{epoch}.pth'))

    os.makedirs(save_dir, exist_ok=True)
    torch.save({
        'model_state': model.state_dict(),
        'optimizer_state': optimizer.state_dict(),
        'epoch': f"{epochs}/{epochs}"
    }, os.path.join(save_dir, f'final_labels{labels_per_class}.pth'))

    return history

def main(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Device:', device)
    os.makedirs(args["save_dir"], exist_ok=True)

    lpc = args["label_count"]

    results = []
    records = {}

    print(f"\n=== Running experiment: {lpc} labels per class ({lpc*10} total) ===\n")
    save_dir = os.path.join(args["save_dir"], f"labels_{lpc}")
    history = run_experiment(
        data_root=args["data_root"],
        labels_per_class=lpc,
        save_dir=save_dir,
        epochs=args["epochs"],
        batch_size=args["batch_size"],
        unlabeled_batch_ratio=args["mu"],
        lambda_u=args["lambda_u"],
        threshold=args.get("threshold", 0.95),
        seed=args.get("seed", 17),
        device=device
    )

    final_acc = history['test_acc'][-1]
    results.append({'labels_per_class': lpc, 'total_labeled': lpc * 10, 'test_acc': final_acc})
    records[lpc] = history

    for th in args["extra_thresholds"]: # para comparar nas avaliaç~eos!! cuidado com tempo gasto
        print(f"--> threshold {th}")
        save_dir = os.path.join(args["save_dir"], f"threshold_{th}")
        history = run_experiment(
            data_root=args["data_root"],
            labels_per_class=lpc,
            save_dir=save_dir,
            epochs=args["epochs"],
            batch_size=args["batch_size"],
            unlabeled_batch_ratio=args["mu"],
            lambda_u=args["lambda_u"],
            threshold=th,
            seed=args.get("seed", 17),
            device=device
        )
        final_acc = history['test_acc'][-1]
        results.append({'labels_per_class': lpc, 'total_labeled': lpc * 10, 'test_acc': final_acc, 'threshold': th})
        records[f'th_{th}'] = history

    # Salva resultados
    df = pd.DataFrame(results)
    df.to_csv(os.path.join(args["save_dir"], 'summary_results.csv'), index=False)

    with open(os.path.join(args["save_dir"], f'histories.pkl'), 'wb') as f:
        pickle.dump(records, f)

    print('\nAll experiments finished. Summary:')
    print(df)


if __name__ == "__main__":
    # @ 400 rótulo por classe
    args = {
        "data_root": "./data",
        "save_dir": "./experiments/fixmatch_cifar10_400labels",
        "label_count": 400,
        "extra_thresholds": [0.8, 0.9],
        "epochs": 27,
        "batch_size": 64,
        "mu": 7,
        "lambda_u": 1.0,
        "threshold": 0.95,
        "seed": 17
    }

    main(args)